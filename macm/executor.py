from utils.gpt_robots import generate_from_excutor
from utils.gpt import Find_Answer_from_GPT
from prompt.prompts import *
import re 

def Execute_steps(conditions,objectives,steps):
    '''
    Ask GPT to Judge the thoughts from the thinker
    Input:
    conditions,objectives,steps (List, List, Str)
    Output:
    final answer (Str)
    '''
    messages = []
    numbered_conditions = "\n".join(f"{i + 1}. {condition}" for i, condition in enumerate(conditions))
    numbered_objective = "\n".join(f"{i + 1}. {objective}" for i, objective in enumerate(objectives))
    message = {
        "role": "user",
        "content": find_target.format(Objective = numbered_objective,
                                      Conditions = numbered_conditions,
                                      Steps = steps)
    }
    messages.append(message)
    message = {
        "role": "user",
        "content": box_target
    }      
    messages.append(message)
    boxed_answer = generate_from_excutor(messages, 
                                         max_tokens = 512, 
                                         model="gpt-4-1106-preview", 
                                         temperature=0.7, n=1)
    return boxed_answer


def Find_Answer(conditions,objectives):
    '''
    ask GPT to Judge the thoughts from the thinker
    Input:
    conditions,objectives,steps (List, List, Str)
    Output:
    final answer (Str)
    '''
    messages = []
    numbered_conditions = "\n".join(f"{i + 1}. {condition}" for i, condition in enumerate(conditions))
    numbered_objective = "\n".join(f"{i + 1}. {objective}" for i, objective in enumerate(objectives))
    message = {
        "role": "user",
        "content": find_target.format(Objective = numbered_objective,Conditions = numbered_conditions)
    }
    messages.append(message)
    final_answer = Find_Answer_from_GPT(messages, max_tokens = 512, 
                                        model="gpt-4-1106-preview", 
                                        temperature=0.7, n=1)
    return final_answer

def simple_request(prompt, cot=False):
    """
    Sends a simple prompt to the model and retrieves the result.

    Args:
        prompt (str): The problem prompt to solve.
        model (Any): The loaded model to perform inference.
        cot (bool): Whether to use a step-by-step reasoning approach.

    Returns:
        str: The result generated by the model.
    """
    try:
        # Prepare prompt for CoT or Base
        if cot:
            cot_prompt = (
                "Solve the following problem using step-by-step reasoning. "
                "Clearly identify intermediate steps and derive the final result.",
                "Your final result should be converted to a float number.\nPlease do not answer anything else, just give the float number marked with \boxed.\n\n"
                f"Problem: {prompt}"
            )
            query = cot_prompt
        else:
            base_prompt = (
                "Solve the following problem and provide the final answer in a boxed format.",
                "Your final result should be converted to a float number.\nPlease do not answer anything else, just give the float number marked with \boxed.\n\n"
                f"Problem: {prompt}"
            )
            query = base_prompt
        result = exec_request(query, max_tokens = 512, 
                                    temperature=0.7, n=1)
                                    
        
        match = re.search(r"\\boxed\{(.*?)\}", result)

        return match.group(1) if match else "No boxed answer found"

    except Exception as e:
        print(f"Error during simple request: {e}")
        return "Error"